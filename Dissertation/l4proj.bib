@article{article,
author = {Ismoilov, Nusrat and Jang, Sung-Bong},
year = {2018},
month = {11},
pages = {648},
title = {A Comparison of Regularization Techniques in Deep Neural Networks},
volume = {10},
journal = {Symmetry},
doi = {10.3390/sym10110648}
}

@article {rule,
	author = {Giles, Oliver and Karlsson, Anneli and Masiala, Spyroula and White, Simon and Cesareni, Gianni and Perfetto, Livia and Mullen, Joe and Hughes, Michael and Harland, Lee and Malone, James},
	title = {Optimising biomedical relationship extraction with BioBERT},
	elocation-id = {2020.09.01.277277},
	year = {2020},
	doi = {10.1101/2020.09.01.277277},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2020/09/01/2020.09.01.277277},
	eprint = {https://www.biorxiv.org/content/early/2020/09/01/2020.09.01.277277.full.pdf},
	journal = {bioRxiv}
}

@misc{mask,
  doi = {10.48550/ARXIV.2010.01923},
  
  url = {https://arxiv.org/abs/2010.01923},
  
  author = {Peng, Hao and Gao, Tianyu and Han, Xu and Lin, Yankai and Li, Peng and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning from Context or Names? An Empirical Study on Neural Relation Extraction},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{logreg,
    title = "Detection of Propaganda Using Logistic Regression",
    author = "Li, Jinfen  and
      Ye, Zhihao  and
      Xiao, Lu",
    booktitle = "Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5017",
    doi = "10.18653/v1/D19-5017",
    pages = "119--124",
}

@article{supervised,
  title={A review of relation extraction},
  author={Bach, Nguyen and Badaskar, Sameer},
  journal={Literature review for Language and Statistics II},
  volume={2},
  pages={1--15},
  year={2007}
}

@article{unsupervised,
  title={An unsupervised text mining method for relation extraction from biomedical literature},
  author={Quan, Changqin and Wang, Meng and Ren, Fuji},
  journal={PloS one},
  volume={9},
  number={7},
  pages={e102039},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@InProceedings{BERT-dropout,
author="El Anigri, Salma
and Himmi, Mohammed Majid
and Mahmoudi, Abdelhak",
editor="Fakir, Mohamed
and Baslam, Mohamed
and El Ayachi, Rachid",
title="How BERT's Dropout Fine-Tuning Affects Text Classification?",
booktitle="Business Intelligence",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="130--139",
abstract="Language models pretraining facilitated fitting models on new and small datasets by keeping the previous pretraining knowledge. The task-agnostic models are to be fine-tuned on all NLP tasks. In this paper, we study the fine-tuning effect of BERT on small amount of data for news classification and sentiment analysis. Our experiments highlight the impact of tweaking the dropout hyper-parameters on the classification performance. We conclude that combining the hidden layers and the attention dropouts probabilities reduce overfitting.",
isbn="978-3-030-76508-8"
}


@article{dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@misc{architectures,
  doi = {10.48550/ARXIV.1906.03158},
  
  url = {https://arxiv.org/abs/1906.03158},
  
  author = {Soares, Livio Baldini and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Matching the Blanks: Distributional Similarity for Relation Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{BERT,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@dataset{dataset,
  author       = {Krallinger, Martin and
                  Rabal, Obdulia and
                  Miranda-Escalada, Antonio and
                  Valencia, Alfonso},
  title        = {{DrugProt corpus: Biocreative VII Track 1 - Text 
                   mining drug and chemical-protein interactions}},
  month        = jun,
  year         = 2021,
  note         = {{DrugProt corpus is promoted by the Plan de Impulso 
                   de las Tecnolog√≠as del Lenguaje de la Agenda
                   Digital (Plan TL).}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.4955411},
  url          = {https://doi.org/10.5281/zenodo.4955411}
}

@article{BioBERT,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    title = "{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}",
    journal = {Bioinformatics},
    volume = {36},
    number = {4},
    pages = {1234-1240},
    year = {2019},
    month = {09},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz682},
    url = {https://doi.org/10.1093/bioinformatics/btz682},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/32527770/btz682.pdf},
}

@article{review,
  title={A review of relation extraction},
  author={Bach, Nguyen and Badaskar, Sameer},
  journal={Literature review for Language and Statistics II},
  volume={2},
  pages={1--15},
  year={2007}
}

@inproceedings{overview,
  title={Overview of DrugProt BioCreative VII track: quality evaluation and large scale text mining of drug-gene/protein relations},
  author={Miranda, Antonio and Mehryary, Farrokh and Luoma, Jouni and Pyysalo, Sampo and Valencia, Alfonso and Krallinger, Martin},
  booktitle={Proceedings of the seventh BioCreative challenge evaluation workshop},
  year={2021}
}

@article{comparison,
	doi = {10.1016/j.websem.2022.100756},
  
	url = {https://doi.org/10.1016\%2Fj.websem.2022.100756},
  
	year = 2023,
	month = {jan},
  
	publisher = {Elsevier {BV}
},
  
	volume = {75},
  
	pages = {100756},
  
	author = {Nikola Milo{\v{s}}evi{\'{c}} and Wolfgang Thielemann},
  
	title = {Comparison of biomedical relationship extraction methods and models for knowledge graph creation},
  
	journal = {Journal of Web Semantics}
}


@article{PubMedBERT,
author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3458754},
doi = {10.1145/3458754},
journal = {ACM Trans. Comput. Healthcare},
month = {oct},
articleno = {2},
numpages = {23},
keywords = {domain-specific pretraining, NLP, Biomedical}
}

@misc{tune,
  doi = {10.48550/ARXIV.1903.05987},
  
  url = {https://arxiv.org/abs/1903.05987},
  
  author = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{tag,
  title={Extracting Drug-Protein Interaction using an Ensemble of Biomedical Pre-trained Language Models through Sequence Labeling and Text Classification Techniques},
  author={Ling Luo and Po-Ting Lai and Chih-Hsuan Wei and Zhiyong Lu},
  year={2021}
}

@inproceedings{humboldt,
  title={Humboldt @ DrugProt: Chemical-Protein Relation Extraction with Pretrained Transformers and Entity Descriptions},
  author={Leon Weber and Mario S{\"a}nger and Samuele Garda and Fabio Barth and Christoph Alt and Ulf Leser},
  year={2021}
}