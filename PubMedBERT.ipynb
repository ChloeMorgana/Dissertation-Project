{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgFnbm129KvOsoeML5lGPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChloeMorgana/Dissertation-Project/blob/main/PubMedBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading of DrugProt dataset\n",
        "\n",
        "Downloading and unzipping file"
      ],
      "metadata": {
        "id": "XEVZhcw6vcub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXtcGw7lujGS",
        "outputId": "e3b06c7e-1277-494c-b36c-76b5e9d3f857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-28 09:16:21--  https://zenodo.org/record/5042151/files/drugprot-gs-training-development.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3796908 (3.6M) [application/octet-stream]\n",
            "Saving to: ‘drugprot-gs-training-development.zip’\n",
            "\n",
            "drugprot-gs-trainin 100%[===================>]   3.62M   407KB/s    in 9.3s    \n",
            "\n",
            "2022-11-28 09:16:32 (399 KB/s) - ‘drugprot-gs-training-development.zip’ saved [3796908/3796908]\n",
            "\n",
            "Archive:  drugprot-gs-training-development.zip\n",
            "   creating: drugprot-gs-training-development/\n",
            "  inflating: drugprot-gs-training-development/README.pdf  \n",
            "   creating: drugprot-gs-training-development/development/\n",
            "  inflating: drugprot-gs-training-development/development/drugprot_development_relations.tsv  \n",
            "  inflating: drugprot-gs-training-development/development/drugprot_development_entities.tsv  \n",
            "  inflating: drugprot-gs-training-development/development/drugprot_development_abstracs.tsv  \n",
            "   creating: drugprot-gs-training-development/training/\n",
            "  inflating: drugprot-gs-training-development/training/drugprot_training_abstracs.tsv  \n",
            "  inflating: drugprot-gs-training-development/training/drugprot_training_relations.tsv  \n",
            "  inflating: drugprot-gs-training-development/training/drugprot_training_entities.tsv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/5042151/files/drugprot-gs-training-development.zip\n",
        "!unzip drugprot-gs-training-development.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining all three files together."
      ],
      "metadata": {
        "id": "BlEsiDFNyVK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDrugProt(abstracts_filename, entities_filename, relations_filename):\n",
        "\n",
        "  docs = {}\n",
        "\n",
        "  # Load the title/abstracts text in as documents\n",
        "  with open(abstracts_filename, encoding='utf8') as f:\n",
        "    for lineno,line in enumerate(f):\n",
        "      split = line.strip('\\n').split('\\t')\n",
        "      assert len(split) == 3, f\"Expected 3 columns but got {len(split)} on line {lineno+1}\"\n",
        "      pubmed_id, title, abstract = split\n",
        "      pubmed_id = int(pubmed_id)\n",
        "\n",
        "      combined_text = title + '\\n' + abstract\n",
        "      docs[pubmed_id] = {'pubmed_id':pubmed_id, 'text':combined_text, 'entities':{}, 'relations':[]}\n",
        "\n",
        "  # Load the entities and match them up with the documents\n",
        "  with open(entities_filename, encoding='utf8') as f:\n",
        "    for lineno,line in enumerate(f):\n",
        "      split = line.strip('\\n').split('\\t')\n",
        "      assert len(split) == 6, f\"Expected 6 columns but got {len(split)} on line {lineno+1}\"\n",
        "      pubmed_id, entity_id, entity_type, start_coord, end_coord, entity_text = split\n",
        "\n",
        "      pubmed_id = int(pubmed_id)\n",
        "      start_coord = int(start_coord)\n",
        "      end_coord = int(end_coord)\n",
        "\n",
        "      assert pubmed_id in docs, f\"Did not find matching document for pubmed_id={pubmed_id}\"\n",
        "      doc = docs[pubmed_id]\n",
        "\n",
        "      assert doc['text'][start_coord:end_coord] == entity_text, f\"Text for entity with coords {start_coord}:{end_coord} in document (pubmed_id={pubmed_id} does not match expected. 'f{doc['text'][start_coord:end_coord]}' != '{entity_text}'\"\n",
        "\n",
        "      entity = {'type':entity_type, 'start':start_coord, 'end':end_coord, 'text':entity_text}\n",
        "      doc['entities'][entity_id] = entity\n",
        "\n",
        "  if relations_filename is not None:\n",
        "    # Load the relations and match them up with the entities in the corresponding document\n",
        "    with open(relations_filename, encoding='utf8') as f:\n",
        "      for lineno,line in enumerate(f):\n",
        "        split = line.strip('\\n').split('\\t')\n",
        "        assert len(split) == 4, f\"Expected 4 columns but got {len(split)} on line {lineno+1}\"\n",
        "\n",
        "        pubmed_id, relation_type, arg1, arg2 = split\n",
        "\n",
        "        pubmed_id = int(pubmed_id)\n",
        "        assert arg1.startswith('Arg1:'), f\"Relation argument should start with 'Arg1:'. Got: {arg1}\"\n",
        "        assert arg2.startswith('Arg2:'), f\"Relation argument should start with 'Arg2:'. Got: {arg2}\"\n",
        "\n",
        "        # Remove arg1/arg2 from text\n",
        "        arg1 = arg1.split(':')[1]\n",
        "        arg2 = arg2.split(':')[1]\n",
        "\n",
        "        assert pubmed_id in docs, f\"Did not find matching document for pubmed_id={pubmed_id}\"\n",
        "        doc = docs[pubmed_id]\n",
        "\n",
        "        assert arg1 in doc['entities'], f\"Couldn't find entity with id={arg1} in document with pubmed_id={pubmed_id}\"\n",
        "        assert arg2 in doc['entities'], f\"Couldn't find entity with id={arg2} in document with pubmed_id={pubmed_id}\"\n",
        "\n",
        "        relation = {'type':relation_type,'arg1':arg1,'arg2':arg2}\n",
        "        doc['relations'].append(relation)\n",
        "\n",
        "  # Convert the dictionary of documents (indexed by pubmed_id) into a simpler dictionary\n",
        "  docs = sorted(docs.values(), key=lambda x:x['pubmed_id'])\n",
        "\n",
        "  return docs"
      ],
      "metadata": {
        "id": "Jnq0pnCUvtPI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying function to dataset"
      ],
      "metadata": {
        "id": "rmXTTkBLyk4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loadDrugProt('drugprot-gs-training-development/training/drugprot_training_abstracs.tsv',\n",
        "                    'drugprot-gs-training-development/training/drugprot_training_entities.tsv',\n",
        "                    'drugprot-gs-training-development/training/drugprot_training_relations.tsv')"
      ],
      "metadata": {
        "id": "yAemM9gmyG7F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation of doc structure\n"
      ],
      "metadata": {
        "id": "hsBqNeznyrJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = [ d for i,d in enumerate(docs) if d['pubmed_id'] == 1280065]\n",
        "assert len(search) == 1, \"Something went wrong and couldn't find the document we want\"\n",
        "\n",
        "doc = search[0]\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYt1O1bLyKK8",
        "outputId": "d1cc48a4-0460-482e-b9a0-890ac5cb735f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pubmed_id': 1280065,\n",
              " 'text': 'Analysis of plasmin binding and urokinase activation of plasminogen bound to the Heymann nephritis autoantigen, gp330.\\nPreviously, we demonstrated that the Heymann nephritis autoantigen, gp330, can serve as a receptor site for plasminogen. This binding was not significantly inhibited by the lysine analogue epsilon-amino caproic acid (EACA), indicating that plasminogen binding was not just through lysine binding sites as suggested for other plasminogen binding sites. We now report that once plasminogen is bound to gp330, it can be converted to its active form of plasmin by urokinase. This conversion of plasminogen to plasmin proceeds at a faster rate when plasminogen is first prebound to gp330. Although there is a proportional increase in the Vmax of the urokinase-catalyzed reaction with increasing gp330 concentrations, no change in Km was observed. Once activated, plasmin remains bound to gp330 in an active state capable of cleaving the chromogenic tripeptide, S-2251. The binding of plasmin to gp330 did not significantly change its enzymatic activity; however, gp330 did have a stabilizing effect on plasmin activity at 37 degrees C. While bound to gp330, plasmin is protected from inactivation by its natural inhibitor alpha 2-antiplasmin. The binding of plasmin to gp330 as analyzed by ELISA was shown to be time dependent, reversible, saturable, and specific for gp330. Inhibition of binding of both plasminogen and plasmin to gp330 by benzamidine was similar, although EACA inhibited the binding of plasmin to gp330 slightly more than the binding of plasminogen to gp330. These results indicate that the binding of plasminogen to gp330 serves as an effective means of increasing the rate of plasmin production on the glomerular and tubular epithelial cell surface while protecting the active plasmin from natural inhibitors.',\n",
              " 'entities': {'T1': {'type': 'CHEMICAL',\n",
              "   'start': 1455,\n",
              "   'end': 1466,\n",
              "   'text': 'benzamidine'},\n",
              "  'T2': {'type': 'CHEMICAL', 'start': 1489, 'end': 1493, 'text': 'EACA'},\n",
              "  'T3': {'type': 'CHEMICAL', 'start': 292, 'end': 298, 'text': 'lysine'},\n",
              "  'T4': {'type': 'CHEMICAL',\n",
              "   'start': 308,\n",
              "   'end': 334,\n",
              "   'text': 'epsilon-amino caproic acid'},\n",
              "  'T5': {'type': 'CHEMICAL', 'start': 336, 'end': 340, 'text': 'EACA'},\n",
              "  'T6': {'type': 'CHEMICAL', 'start': 400, 'end': 406, 'text': 'lysine'},\n",
              "  'T7': {'type': 'GENE-Y', 'start': 1165, 'end': 1170, 'text': 'gp330'},\n",
              "  'T8': {'type': 'GENE-Y', 'start': 1172, 'end': 1179, 'text': 'plasmin'},\n",
              "  'T9': {'type': 'GENE-Y', 'start': 227, 'end': 238, 'text': 'plasminogen'},\n",
              "  'T10': {'type': 'GENE-Y',\n",
              "   'start': 1236,\n",
              "   'end': 1255,\n",
              "   'text': 'alpha 2-antiplasmin'},\n",
              "  'T11': {'type': 'GENE-Y', 'start': 1272, 'end': 1279, 'text': 'plasmin'},\n",
              "  'T12': {'type': 'GENE-Y', 'start': 1283, 'end': 1288, 'text': 'gp330'},\n",
              "  'T13': {'type': 'GENE-Y', 'start': 1382, 'end': 1387, 'text': 'gp330'},\n",
              "  'T14': {'type': 'GENE-Y', 'start': 1419, 'end': 1430, 'text': 'plasminogen'},\n",
              "  'T15': {'type': 'GENE-Y', 'start': 1435, 'end': 1442, 'text': 'plasmin'},\n",
              "  'T16': {'type': 'GENE-Y', 'start': 1446, 'end': 1451, 'text': 'gp330'},\n",
              "  'T17': {'type': 'GENE-Y', 'start': 1519, 'end': 1526, 'text': 'plasmin'},\n",
              "  'T18': {'type': 'GENE-Y', 'start': 1530, 'end': 1535, 'text': 'gp330'},\n",
              "  'T19': {'type': 'GENE-Y', 'start': 1570, 'end': 1581, 'text': 'plasminogen'},\n",
              "  'T20': {'type': 'GENE-Y', 'start': 1585, 'end': 1590, 'text': 'gp330'},\n",
              "  'T21': {'type': 'GENE-Y', 'start': 1635, 'end': 1646, 'text': 'plasminogen'},\n",
              "  'T22': {'type': 'GENE-Y', 'start': 1650, 'end': 1655, 'text': 'gp330'},\n",
              "  'T23': {'type': 'GENE-Y', 'start': 1711, 'end': 1718, 'text': 'plasmin'},\n",
              "  'T24': {'type': 'GENE-Y', 'start': 1812, 'end': 1819, 'text': 'plasmin'},\n",
              "  'T25': {'type': 'GENE-Y', 'start': 359, 'end': 370, 'text': 'plasminogen'},\n",
              "  'T26': {'type': 'GENE-Y', 'start': 444, 'end': 455, 'text': 'plasminogen'},\n",
              "  'T27': {'type': 'GENE-Y', 'start': 495, 'end': 506, 'text': 'plasminogen'},\n",
              "  'T28': {'type': 'GENE-Y',\n",
              "   'start': 156,\n",
              "   'end': 185,\n",
              "   'text': 'Heymann nephritis autoantigen'},\n",
              "  'T29': {'type': 'GENE-Y', 'start': 519, 'end': 524, 'text': 'gp330'},\n",
              "  'T30': {'type': 'GENE-Y', 'start': 568, 'end': 575, 'text': 'plasmin'},\n",
              "  'T31': {'type': 'GENE-Y', 'start': 579, 'end': 588, 'text': 'urokinase'},\n",
              "  'T32': {'type': 'GENE-Y', 'start': 609, 'end': 620, 'text': 'plasminogen'},\n",
              "  'T33': {'type': 'GENE-Y', 'start': 624, 'end': 631, 'text': 'plasmin'},\n",
              "  'T34': {'type': 'GENE-Y', 'start': 663, 'end': 674, 'text': 'plasminogen'},\n",
              "  'T35': {'type': 'GENE-Y', 'start': 696, 'end': 701, 'text': 'gp330'},\n",
              "  'T36': {'type': 'GENE-Y', 'start': 764, 'end': 773, 'text': 'urokinase'},\n",
              "  'T37': {'type': 'GENE-Y', 'start': 187, 'end': 192, 'text': 'gp330'},\n",
              "  'T38': {'type': 'GENE-Y', 'start': 809, 'end': 814, 'text': 'gp330'},\n",
              "  'T39': {'type': 'GENE-Y', 'start': 877, 'end': 884, 'text': 'plasmin'},\n",
              "  'T40': {'type': 'GENE-Y', 'start': 902, 'end': 907, 'text': 'gp330'},\n",
              "  'T41': {'type': 'GENE-Y', 'start': 998, 'end': 1005, 'text': 'plasmin'},\n",
              "  'T42': {'type': 'GENE-Y', 'start': 1009, 'end': 1014, 'text': 'gp330'},\n",
              "  'T43': {'type': 'GENE-Y', 'start': 1077, 'end': 1082, 'text': 'gp330'},\n",
              "  'T44': {'type': 'GENE-Y', 'start': 1116, 'end': 1123, 'text': 'plasmin'},\n",
              "  'T45': {'type': 'GENE-Y', 'start': 112, 'end': 117, 'text': 'gp330'},\n",
              "  'T46': {'type': 'GENE-Y', 'start': 12, 'end': 19, 'text': 'plasmin'},\n",
              "  'T47': {'type': 'GENE-Y', 'start': 32, 'end': 41, 'text': 'urokinase'},\n",
              "  'T48': {'type': 'GENE-Y', 'start': 56, 'end': 67, 'text': 'plasminogen'},\n",
              "  'T49': {'type': 'GENE-Y',\n",
              "   'start': 81,\n",
              "   'end': 110,\n",
              "   'text': 'Heymann nephritis autoantigen'}},\n",
              " 'relations': [{'type': 'INHIBITOR', 'arg1': 'T1', 'arg2': 'T14'},\n",
              "  {'type': 'INHIBITOR', 'arg1': 'T1', 'arg2': 'T15'},\n",
              "  {'type': 'DIRECT-REGULATOR', 'arg1': 'T1', 'arg2': 'T16'},\n",
              "  {'type': 'INHIBITOR', 'arg1': 'T2', 'arg2': 'T17'},\n",
              "  {'type': 'DIRECT-REGULATOR', 'arg1': 'T2', 'arg2': 'T18'},\n",
              "  {'type': 'INHIBITOR', 'arg1': 'T2', 'arg2': 'T19'},\n",
              "  {'type': 'DIRECT-REGULATOR', 'arg1': 'T2', 'arg2': 'T20'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total number of relations"
      ],
      "metadata": {
        "id": "SvJVGoiznqJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = 0\n",
        "for doc in docs:\n",
        "  num += len(doc['relations'])\n",
        "\n",
        "print(num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaUZTJ6Qntfq",
        "outputId": "798e3a33-fad4-48d7-f50b-78f6984740fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Step"
      ],
      "metadata": {
        "id": "wuvPifsbVAI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch\n",
        "!pip install transformers\n",
        "!pip install segtok\n",
        "\n",
        "#import transformers as ppb\n",
        "from transformers import BertModel, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "from segtok.segmenter import split_single"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt-G3qMF6S6z",
        "outputId": "fa0dc653-a9cf-400e-d00b-0acb8187c32e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (1.5.11)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding in start and end entity tags to biomedical text, e.g. [E1] CHEMICAL [/E1] and [E2] GENE [/E2]"
      ],
      "metadata": {
        "id": "DjiNDFJvgckl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isRelation(doc, key):\n",
        "  relations = doc['relations']\n",
        "  entities = doc['entities'].keys()\n",
        "  for r in relations:\n",
        "    if (key == r['arg1']) or (key == r['arg2']):\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "ruR21j2mXzO8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addEntityTags(docs):\n",
        "\n",
        "  for doc in docs:\n",
        "    doc['labels'] = []\n",
        "    str = ''\n",
        "    index = 0\n",
        "    n = 1\n",
        "    keys = doc['entities'].keys()\n",
        "    key_list = sorted(keys, key= lambda x: (doc['entities'][x]['start']))\n",
        "    for k in key_list:\n",
        "      if isRelation(doc, k):\n",
        "        doc['labels'].append(1)\n",
        "      else:\n",
        "        doc['labels'].append(0)\n",
        "      v = doc['entities'][k]\n",
        "      start = v['start']\n",
        "      end = v['end']\n",
        "      name = v['text']\n",
        "      if v['type'] == 'CHEMICAL':\n",
        "        str += doc['text'][index:start] + f'[E1] ' + doc['text'][start:end] + f' [/E1]'\n",
        "      else:\n",
        "        str += doc['text'][index:start] + f'[E2] ' + doc['text'][start:end] + f' [/E2]'\n",
        "      index = end\n",
        "    doc['tagged_text'] = str \n",
        "\n",
        "addEntityTags(docs)\n",
        "print(docs[0]['tagged_text'])\n",
        "print(len(docs[0]['labels']))\n",
        "      "
      ],
      "metadata": {
        "id": "a7MjEIFqgiiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a18279-5587-4769-cfca-b1cd022f3c28"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis of [E2] plasmin [/E2] binding and [E2] urokinase [/E2] activation of [E2] plasminogen [/E2] bound to the [E2] Heymann nephritis autoantigen [/E2], [E2] gp330 [/E2].\n",
            "Previously, we demonstrated that the [E2] Heymann nephritis autoantigen [/E2], [E2] gp330 [/E2], can serve as a receptor site for [E2] plasminogen [/E2]. This binding was not significantly inhibited by the [E1] lysine [/E1] analogue [E1] epsilon-amino caproic acid [/E1] ([E1] EACA [/E1]), indicating that [E2] plasminogen [/E2] binding was not just through [E1] lysine [/E1] binding sites as suggested for other [E2] plasminogen [/E2] binding sites. We now report that once [E2] plasminogen [/E2] is bound to [E2] gp330 [/E2], it can be converted to its active form of [E2] plasmin [/E2] by [E2] urokinase [/E2]. This conversion of [E2] plasminogen [/E2] to [E2] plasmin [/E2] proceeds at a faster rate when [E2] plasminogen [/E2] is first prebound to [E2] gp330 [/E2]. Although there is a proportional increase in the Vmax of the [E2] urokinase [/E2]-catalyzed reaction with increasing [E2] gp330 [/E2] concentrations, no change in Km was observed. Once activated, [E2] plasmin [/E2] remains bound to [E2] gp330 [/E2] in an active state capable of cleaving the chromogenic tripeptide, S-2251. The binding of [E2] plasmin [/E2] to [E2] gp330 [/E2] did not significantly change its enzymatic activity; however, [E2] gp330 [/E2] did have a stabilizing effect on [E2] plasmin [/E2] activity at 37 degrees C. While bound to [E2] gp330 [/E2], [E2] plasmin [/E2] is protected from inactivation by its natural inhibitor [E2] alpha 2-antiplasmin [/E2]. The binding of [E2] plasmin [/E2] to [E2] gp330 [/E2] as analyzed by ELISA was shown to be time dependent, reversible, saturable, and specific for [E2] gp330 [/E2]. Inhibition of binding of both [E2] plasminogen [/E2] and [E2] plasmin [/E2] to [E2] gp330 [/E2] by [E1] benzamidine [/E1] was similar, although [E1] EACA [/E1] inhibited the binding of [E2] plasmin [/E2] to [E2] gp330 [/E2] slightly more than the binding of [E2] plasminogen [/E2] to [E2] gp330 [/E2]. These results indicate that the binding of [E2] plasminogen [/E2] to [E2] gp330 [/E2] serves as an effective means of increasing the rate of [E2] plasmin [/E2] production on the glomerular and tubular epithelial cell surface while protecting the active [E2] plasmin [/E2]\n",
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing the text only to sentences potentially containing relations between chemicals and genes:"
      ],
      "metadata": {
        "id": "4ikJplM-cXqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def relevantText():\n",
        "\n",
        "  target_text = []\n",
        "  labels = []\n",
        "\n",
        "  for doc in docs:\n",
        "    txt = doc['tagged_text']\n",
        "    lbls = doc['labels']\n",
        "\n",
        "    split_text = []\n",
        "    split_text += split_single(txt)\n",
        "    offset = 0\n",
        "\n",
        "    for s in split_text:\n",
        "      if '[/E' in s:\n",
        "        o, l, new_lbls = findLabels(s,lbls,offset)\n",
        "        target_text.append(s)\n",
        "        lbls = new_lbls\n",
        "        offset += o\n",
        "        labels.append(l)\n",
        "\n",
        "  return target_text, labels\n",
        "\n",
        "def findLabels(text, labels, offset=0):\n",
        "    words = text.split(\" \")\n",
        "    counter = Counter(words)\n",
        "    idx = counter['[E2]']+counter['[E1]']\n",
        "    isRel = False\n",
        "    #print(labels)\n",
        "    #print(idx)\n",
        "    for i in range(idx):\n",
        "      if labels[i]==1:\n",
        "        isRel = True\n",
        "    offset+=idx\n",
        "    if isRel:\n",
        "      label = 1\n",
        "    else:\n",
        "      label = 0\n",
        "    l = labels[idx:]\n",
        "    return offset, label, l\n",
        "\n",
        "target_text, labels= relevantText()\n",
        "t = target_text[:49]\n",
        "l = labels[:49]\n",
        "\n",
        "print(len(target_text))\n",
        "print(len(labels))\n",
        "print(l)\n"
      ],
      "metadata": {
        "id": "x0ps_RaP3wMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b040e2-000e-4f64-af7a-8cebe9dd8216"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29234\n",
            "29234\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are around 17288 total relations, therefore there some sentences that contain entities but don't actually contain a relation"
      ],
      "metadata": {
        "id": "jI2VTA9Po-a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(target_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb4HIf7ebZdo",
        "outputId": "c5faf588-653a-4485-da89-1cbec0f736b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Step\n",
        "\n",
        "Loading pre-trained PubMedBERT model + tokenizer"
      ],
      "metadata": {
        "id": "BXYgjLzmG2DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
        "                                  output_hidden_states = True,\n",
        "                                  )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\") \n",
        "\n",
        "tokenizer.add_tokens([\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"])\n",
        "\n",
        "tokenizer.vocab[\"[E1]\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtHyA7DTG40c",
        "outputId": "06220a71-fbea-4fc5-ea0a-b555f9d49af0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding more tags and tokenising input"
      ],
      "metadata": {
        "id": "eFzXfZX-X3D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_text_prep(text, tokenizer):\n",
        "\n",
        "  #CLS lets BERT know when start of sentence begins, SEP indicates start of second sentence.\n",
        "  marked_text = \"[CLS]\" + text + \"[SEP]\"\n",
        "\n",
        "  #Tokenizes text\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "  #Converts to ids\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  segments_ids = [1]*len(indexed_tokens)\n",
        "\n",
        "  # Converts inputs to tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  return tokenized_text, tokens_tensor, segments_tensors"
      ],
      "metadata": {
        "id": "8LWbqPbBL6TD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting input into embeddings"
      ],
      "metadata": {
        "id": "PMth1UIhX6a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    hidden_states = outputs[2][1:]\n",
        "\n",
        "  token_embeddings = hidden_states[-1]\n",
        "  #Collapsing tensor into 1 dimension\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
        "  #converting tensors to lists\n",
        "  list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "\n",
        "  return list_token_embeddings"
      ],
      "metadata": {
        "id": "-OnSsiEiOQrM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting CLS embeddings for each sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "SDwF0f2a3rHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = target_text[:5]\n",
        "cls_embeddings = []\n",
        "\n",
        "for text in t:\n",
        "    tokenized_text, tokens_tensor, segments_tensors = bert_text_prep(text, tokenizer)\n",
        "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "    cls_embeddings.append(list_token_embeddings[0])\n",
        "\n",
        "print(len(cls_embeddings))"
      ],
      "metadata": {
        "id": "tXoMT_blWrhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ec2793-1c23-4732-cbec-111224368c06"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Step"
      ],
      "metadata": {
        "id": "E4CWwTfCcIqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevant imports:\n",
        "\n"
      ],
      "metadata": {
        "id": "ijHn5tYH2ecP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "CwZpVx6Y9Vs-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the embeddings into train/test sets"
      ],
      "metadata": {
        "id": "TrzGuFGnECwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(cls_embeddings, l)"
      ],
      "metadata": {
        "id": "fZqHekZbEHon"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying these using logistic regression"
      ],
      "metadata": {
        "id": "mbGMU_aTMd0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "lr_clf.score(test_features, test_labels)"
      ],
      "metadata": {
        "id": "2Puvz2iJMkS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757fd569-fc96-419c-dfe9-a74528aa73d5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6923076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establishing the relation labels"
      ],
      "metadata": {
        "id": "4lZZpVkhdC7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(set([r['type'] for doc in docs for r in doc['relations']]))"
      ],
      "metadata": {
        "id": "QklH7WB6ZaWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}