# Work Done
* Changed BERT dropout parameters and experienced a negligible change so far
* Tried entity masking and discovered no difference compared to entity marking so far
* Added more to my dissertation

# Findings
* I'm tempted to try and see if I can implement either a linear layer
* I'm making good progress with my dissertation
* I think it would be interesting to compare feature extraction with fine-tuning
* Adam is an optimization algorithm that can handle sparse gradients on noisy problems.
* It is my understanding that for every epoch, the learning rate scheduler changes the learning rate (step size) with the goal of minimising the error of the model.

# Questions
* How should I adapt my research questions to accomodate fine-tuning?
* May I have the slides from Wednesday?
* Do you have any good resources for implementing a linear layer?
